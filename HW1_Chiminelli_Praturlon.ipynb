{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOIaEvWVUKFp"
      },
      "source": [
        "# ID2222 Data Mining, Homework 1 \n",
        "# **Finding Similar Items: Textually Similar Documents**\n",
        "\n",
        "Brando Chiminelli, Tommaso Praturlon\n",
        "\n",
        "November 14th, 2022"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PqOxlll3C1p"
      },
      "source": [
        "The goal of this notebook is to implement the stages of finding textually similar\n",
        "documents based on Jaccard similarity and an estimation of it.\n",
        "In particular, the following techniques are addressing the problem by appying\n",
        "the functions one after the other, like in a pipeline.\n",
        "These functions are called, in order, Shingling, MinHashing,\n",
        "and Locality-Sensitive Hashing (LSH)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35iKgnWG3C1q"
      },
      "source": [
        "## Import Libraries and read a sample from the file\n",
        "\n",
        "We are reading a sample of 100 documents (article texts) from a dataset of news articles available\n",
        "on Kaggle (https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset).\n",
        "In order to run this notebook you need to import the dataset in the same directory\n",
        "of this notebook.\n",
        "Here, we are also dropping any possible duplicates because this would alter the similarity analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsKXTmhdUKFq",
        "outputId": "0037249a-d66a-4394-973b-771f08c839bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data read successfully!\n",
            "Number of documents: \n",
            "100\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "PATH_TO_DATA = \"True.csv\"\n",
        "df = pd.read_csv(PATH_TO_DATA)\n",
        "print(\"Data read successfully!\")\n",
        "# Delete duplicates from the dataset in the columns title and text\n",
        "df.drop_duplicates(subset=['text'], keep='first', inplace=True)\n",
        "\n",
        "NUM_OF_DOCS = 100\n",
        "print(\"Number of documents: \")\n",
        "print(NUM_OF_DOCS)\n",
        "\n",
        "# Select random documents from the dataset\n",
        "data_ls = random.sample(list(df['text']),NUM_OF_DOCS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idR-zhr63C1t"
      },
      "source": [
        "## Shingling\n",
        "\n",
        "The following two functions are implementing the first stage of our pipeline: **Shingling**\n",
        "\n",
        "A document is divided into sequences of characters of length k, called shingles. A document is then the set of its k-shingles. The underlying assumption is that documents with lots of shingles in common are similar. This similarity can be identified with the Jaccard Similarity rule.\n",
        "\n",
        "When shingles are long, their length can be reduced using hash functions, thus representing the document with the set of its hash values for those k-shingles.\n",
        "\n",
        "This is performed in the following two functions. The first, given a text document, returns a set of its hashed shingles.\n",
        "\n",
        "With the function list_shingles(), we are building the list of sets of hashed shingles for each document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "pfPtkGmOUKFr"
      },
      "outputs": [],
      "source": [
        "K_SHINGLE = 5\n",
        "\n",
        "def shingles(doc, k=K_SHINGLE):\n",
        "    '''\n",
        "    Description: takes a document as input and returns a set of the shingles for that document.\n",
        "    shingles_ch are the hashed shingles using built-in hash()\n",
        "    shingles_c are only characters\n",
        "    '''\n",
        "    shingles_hash = [hash(doc[i: i+k]) for i in range(len(doc) - k + 1)]\n",
        "\n",
        "    return set(shingles_hash)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "4BsqtXwFUKFr"
      },
      "outputs": [],
      "source": [
        "def list_shingles(doc_ls, k=K_SHINGLE):\n",
        "    '''\n",
        "    Description: takes as input all the documents as a list and returns a list of\n",
        "    all hased shingles for each document.\n",
        "    '''\n",
        "    shingle_list = [] # list of sets of hashed shingles for each document\n",
        "    for doc in doc_ls:\n",
        "        s_hash = shingles(doc,k)\n",
        "        shingle_list.append(s_hash)\n",
        "\n",
        "    return shingle_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJMBCPDYUKFr",
        "outputId": "584eccbe-531f-464b-bc81-96563e233e5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shingle list length: \n",
            "100\n"
          ]
        }
      ],
      "source": [
        "shingles_list = list_shingles(data_ls)\n",
        "print(\"Shingle list length: \")\n",
        "print(len(shingles_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1jQ3Uik3C1z"
      },
      "source": [
        "### Compare two sets of shingles using Jaccard Similarity\n",
        "\n",
        "The next function computes the Jaccard Similarity of two set of hashed shingles by dividing the cardinality of the intersection of two sets for the cardinality of their union."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "foGLrsGfUKFr"
      },
      "outputs": [],
      "source": [
        "def jaccard_similarity(set1, set2):\n",
        "\n",
        "    intersection = len(list(set1.intersection(set2)))\n",
        "    union = (len(set1) + len(set2)) - intersection\n",
        "\n",
        "    return float(intersection) / union"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnJfRyMi3C10"
      },
      "source": [
        "Now we want to print out a matrix of the similarities to get an overall view of our dataset, the main diagonal will of course be all ones as we are computing the comparison of two sets that are the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rCH60Iy3C11",
        "outputId": "0d602d8a-fbac-40ce-93d8-ea19eef2a7f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.         0.06903648 0.03608737 ... 0.08120722 0.04491932 0.0373917 ]\n",
            " [0.06903648 0.         0.0393404  ... 0.09291188 0.05351856 0.03515625]\n",
            " [0.03608737 0.0393404  0.         ... 0.04070881 0.07360157 0.0329087 ]\n",
            " ...\n",
            " [0.08120722 0.09291188 0.04070881 ... 0.         0.0623053  0.04472107]\n",
            " [0.04491932 0.05351856 0.07360157 ... 0.0623053  0.         0.04031551]\n",
            " [0.0373917  0.03515625 0.0329087  ... 0.04472107 0.04031551 0.        ]]\n",
            "Total 20\n"
          ]
        }
      ],
      "source": [
        "# Create the Similarity Matrix, storing the percentage of similarity document by document\n",
        "simil_mat = np.zeros((len(shingles_list),len(shingles_list)))\n",
        "# Compute the Jaccard Similarity\n",
        "total = 0\n",
        "for i in range(len(shingles_list)):\n",
        "    for j in range(len(shingles_list)):\n",
        "      simil_mat[i,j] = jaccard_similarity(shingles_list[i],shingles_list[j])\n",
        "      if i==j: simil_mat[i,j] = 0 # to remove ones in diagonal\n",
        "      if simil_mat[i,j]>=0.14: total +=1\n",
        "\n",
        "print(simil_mat)\n",
        "print(\"Total\", total)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSeDJdYx3C12"
      },
      "source": [
        "## MinHashing\n",
        "\n",
        "The following two function are implementing the second stage of our pipeline: **MinHashing**\n",
        "\n",
        "The idea is to compute, for each set (shingles) inside shingles_list, the hash function for each shingle in the set and take the one with minimum value.\n",
        "\n",
        "This is performed in the class minhash, which, given the list of shingles and the number of hash functions (k), returns a matrix, where for each different hash function (one different applied in every row) we take the minimum hashes via the hash function h(x) = (x*a + b)%c for every set of shingles and we do that for every element of the shingles_list so for every document (columns). \n",
        "\n",
        "In the hashFunction, a and b are random numbers and c is the maximum prime integer representable in 32 bits.\n",
        "\n",
        "The matrix that is return is called signatures because it is the signature matrix where each column is the signature of one document. In this way we have compressed the shingles matrix and we can compute the similarity, which will be an estimation of the Jaccard similarity we saw before, but will be of a lower complexity, depending on the number of hash function applied.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "saab897EUKFr"
      },
      "outputs": [],
      "source": [
        "K_MINHASH = 100 # number of hash functions\n",
        "\n",
        "def hashFunction(k):\n",
        "    '''\n",
        "    Creates the coefficients for the hash function\n",
        "    h(x) = (ax+b)%c\n",
        "    '''\n",
        "    a = np.random.rand(k)\n",
        "    b = np.random.rand(k)\n",
        "    c = 4294967311 # maximum integer\n",
        "\n",
        "    return a,b,c # a,b are arrays of rand number between [0, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "hEGT7ADF3C13"
      },
      "outputs": [],
      "source": [
        "def minhash(shingles_list, k=K_MINHASH):\n",
        "    '''\n",
        "    Takes as input a list of hashed shingles and returns the Signature Matrix.\n",
        "    '''\n",
        "    # Signature Matrix, rows: k hash functions,\n",
        "    # cols: list of sets of hashed shingles for each document\n",
        "    signatures_mat = np.zeros((k, len(shingles_list)))\n",
        "    a, b, c = hashFunction(k)\n",
        "    # print(a,b,c)\n",
        "\n",
        "    # for every hash function\n",
        "    for h_fun in range(k):\n",
        "      # for every set of shingles in the list of all shingles\n",
        "      for shingles in shingles_list:\n",
        "        # apply the same hash function to every set of shingles (to every document's shingles)\n",
        "        hashCode = map(lambda shingle: (shingle*a[h_fun] + b[h_fun])%c, shingles)\n",
        "        minHashCode = min(hashCode) # take the minimum value for that set of hashed shingles\n",
        "        # build Signature Matrix, row: same hash function,\n",
        "        # cols: minimum value for that set at its index inside the list\n",
        "        signatures_mat[h_fun, shingles_list.index(shingles)] = minHashCode\n",
        "\n",
        "    return signatures_mat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PB1-U-fO3C13",
        "outputId": "b02e7234-180a-4a6d-f7e4-67b6b84827a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[  40406. 2816330. 1634594. ... 2171184.  865870. 1634594.]\n",
            " [3096769.  679338.  555276. ...  679338.  566275.  756264.]\n",
            " [2067712.   46596. 2932944. ...   46596.   46596.   46596.]\n",
            " ...\n",
            " [3958234. 2051660. 2051660. ... 3688837. 4055646. 1182714.]\n",
            " [3055155. 5162252. 1466002. ... 1722447. 2085211. 1309133.]\n",
            " [1135225.  885962. 2651588. ... 2310777. 1610693. 1957538.]]\n"
          ]
        }
      ],
      "source": [
        "signatures_mat = minhash(shingles_list)\n",
        "print(signatures_mat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnaWSrOE3C14"
      },
      "source": [
        "### Compare two sets of signatures, estimation of Jaccard Similarity\n",
        "\n",
        "As anticipated, in the next funtion we are computing the estimation of the Jaccard similarity between pairs of signatures. We can tell from the output printed that we have a decrease in accuray from the comparison between pairs of shingles, but this also comes with an improve in the performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "ZD6CnlhI3C15"
      },
      "outputs": [],
      "source": [
        "def compare_signatures(signature1, signature2, k=K_MINHASH):\n",
        "    '''\n",
        "    signature1 is a set of minhashed shingles of the first document we want to compare\n",
        "    k is the number of minhash functions used so the number of rows in the signature matrix\n",
        "    the comparison of signatures is an estimation of jaccard similarity\n",
        "    '''\n",
        "    intersection = len(list(signature1.intersection(signature2)))\n",
        "\n",
        "    return float(intersection) / k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d9pTzXi3C15",
        "outputId": "5efa11c1-dbbe-4503-e790-87154ad22bad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Compared Signatures\n",
            "[[0.   0.01 0.03 ... 0.06 0.01 0.02]\n",
            " [0.01 0.   0.04 ... 0.08 0.11 0.1 ]\n",
            " [0.03 0.04 0.   ... 0.08 0.05 0.06]\n",
            " ...\n",
            " [0.06 0.08 0.08 ... 0.   0.04 0.05]\n",
            " [0.01 0.11 0.05 ... 0.04 0.   0.11]\n",
            " [0.02 0.1  0.06 ... 0.05 0.11 0.  ]]\n"
          ]
        }
      ],
      "source": [
        "# Build Similarity Matrix for signatures\n",
        "simil_mat_sign = np.zeros((len(signatures_mat),len(signatures_mat)))\n",
        "\n",
        "# Compare each minHashed value from the same function with all others in same row\n",
        "for i in range(len(signatures_mat)):\n",
        "    for j in range(len(signatures_mat)):\n",
        "      simil_mat_sign[i,j] = compare_signatures(set(signatures_mat[:,i]),set(signatures_mat[:,j]), K_MINHASH)\n",
        "      if i==j: simil_mat_sign[i,j] = 0 # to remove ones in diagonal\n",
        "\n",
        "print(\"Compared Signatures\")\n",
        "print(simil_mat_sign)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZULozAj43C16"
      },
      "source": [
        "## Locality-Sensitive Hashing\n",
        "\n",
        "The third part of the implementation consists in Locality-Sensitive Hashing. Function band_hashing() creates the bands, get_bucket_list() creates the list of buckets, query_band_hashing() makes a list of hash functions for each bucket and find_similar_docs() uses the previous functions to find similar documents using LSH."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "8Wuva3x58SCU"
      },
      "outputs": [],
      "source": [
        "def band_hashing(band, hash_f, buckets_dict):\n",
        "    '''\n",
        "    This function takes a band as input,\n",
        "    hashes it and puts it in the buckets_list\n",
        "    at its respective postition.\n",
        "    buckets_dict is a dictionary with key: the hash value, and value: the indexes of the col\n",
        "    '''\n",
        "\n",
        "    for col in band.columns:\n",
        "        h = hash_f(tuple(band[col].values))\n",
        "        if h in buckets_dict:\n",
        "            buckets_dict[h].append(col)\n",
        "        else:\n",
        "            buckets_dict[h] = [col]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "vuohmkvl8Xj7"
      },
      "outputs": [],
      "source": [
        "def get_bucket_list(sign_mat, r, hash_f=None):\n",
        "    '''\n",
        "    This function returns the list of buckets with similar documents\n",
        "    This function generates the list of dictionaries objects where\n",
        "    each band is hashed to a bucket in a dictionary.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    sign_mat: pandas.Dataframe\n",
        "        signatures of all the documents generated from minhashing\n",
        "    r: int\n",
        "        no of rows in each band\n",
        "    hash_f: function, optional\n",
        "        hash function used to hash document to buckets\n",
        "    Returns\n",
        "    -------\n",
        "    buckets_list: a list of dictionaries. Each dictionary\n",
        "        contains hashes of column vectors of the band as keys\n",
        "        and the list of documents as values.\n",
        "    '''\n",
        "    # b: number of bands\n",
        "    # n: length of a document signature = numebr of minhash functions used\n",
        "    # r: number of rows in a band\n",
        "    n = sign_mat.shape[0]\n",
        "    b = n//r # floor division\n",
        "    buckets_list = [dict() for i in range(b)]\n",
        "\n",
        "    if hash_f == None:\n",
        "        hash_f = hash\n",
        "\n",
        "    for i in range(0, n-r+1, r):\n",
        "        band = sign_mat.loc[i:i+r-1,:] # access group of rows\n",
        "        band_hashing(band, hash_f, buckets_list[int(i/r)])\n",
        "\n",
        "    return buckets_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "xpXOCkyg8hQu"
      },
      "outputs": [],
      "source": [
        "def query_band_hashing(band, hash_f):\n",
        "    '''\n",
        "    This function takes a band of query document as input,\n",
        "    hashes it and puts it in the buckets_list\n",
        "    at its respective postition.\n",
        "    '''\n",
        "\n",
        "    hash_list = []\n",
        "    h = hash_f(tuple(band.values))\n",
        "    hash_list.append(h)\n",
        "\n",
        "    return hash_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "u6LprZxH8l--"
      },
      "outputs": [],
      "source": [
        "def find_similar_docs(doc_id, buckets_list, sign_mat, r, hash_f=None):\n",
        "    '''\n",
        "    This function finds similar documents\n",
        "    Parameters\n",
        "    ----------\n",
        "    buckets_list: list\n",
        "        list of dictionary objects generated by get_bucket_list\n",
        "    hash_f: function, optional\n",
        "        the same hash function used for get_bucket_list\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    set\n",
        "        set containing similar documents to given document\n",
        "        ADD SIMILARITY THRESHOLD\n",
        "    '''\n",
        "\n",
        "    # b: number of bands\n",
        "    # n: length of a document signature\n",
        "    # r: number of rows in a band\n",
        "    n = sign_mat.shape[0]\n",
        "    b = n//r\n",
        "\n",
        "    if hash_f == None:\n",
        "        hash_f = hash\n",
        "\n",
        "    query_bucket_list = []\n",
        "\n",
        "    for i in range(0, n-r+1, r):\n",
        "        band = sign_mat.loc[i:i+r-1, int(doc_id)]\n",
        "        query_bucket_list.append(query_band_hashing(band, hash_f))\n",
        "\n",
        "    similar_docs = set()\n",
        "    for i in range(len(query_bucket_list)):\n",
        "        for j in range(len(query_bucket_list[i])):\n",
        "            similar_docs.update(set(buckets_list[i][query_bucket_list[i][j]]))\n",
        "\n",
        "    return similar_docs # list of similar documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KM05KT4w8raj",
        "outputId": "fdf45cb9-ae7c-49f6-f9af-d06c3707ff4c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{2}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cols = [] # get the indexes of colums for the new data frame\n",
        "for i in range(100):\n",
        "    cols.append(i)\n",
        "\n",
        "df_signatures = pd.DataFrame(signatures_mat, columns = cols) # convert Signatures Matrix to a data frame\n",
        "\n",
        "buckets_list = get_bucket_list(df_signatures, 5)\n",
        "\n",
        "doc_id = 2\n",
        "# gives out the set of documents id that are similar to the one we pass in d\n",
        "find_similar_docs(doc_id, buckets_list, df_signatures, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzLPmAVZBPRS"
      },
      "source": [
        "## Performance Analysis\n",
        "\n",
        "Here the intention is to test and evaluate the implementation's scalability (the execution time versus the size of the input dataset).\n",
        "The similarity threshold is set to 0.14 and it is empirically obtained since the similarities in the dataset are low above 10%.\n",
        "\n",
        "The execution time of every function will be considered and plotted in the graph. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "Fi6c9vA-CFsk"
      },
      "outputs": [],
      "source": [
        "# For cycle iterating a vector of N documents\n",
        "# Calling various functions\n",
        "# Saving each function duration in its vector of times\n",
        "# Plot these vectors versus the number of documents.\n",
        "\n",
        "n_of_docs = [10, 100, 500, 1000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ud9Oj49aNDMo"
      },
      "source": [
        "### Shingling Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwV7-l9kEbMY",
        "outputId": "bb8a4a47-1d33-4556-e11d-82a4a34263e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---\n",
            "Number of documents:  10 \n",
            "Similar documents:  0 \n",
            "Time spent:  0.023193359375\n",
            "---\n",
            "Number of documents:  100 \n",
            "Similar documents:  6 \n",
            "Time spent:  0.8510866165161133\n",
            "---\n",
            "Number of documents:  500 \n",
            "Similar documents:  202 \n",
            "Time spent:  20.291604042053223\n",
            "---\n",
            "Number of documents:  1000 \n",
            "Similar documents:  888 \n",
            "Time spent:  72.83159065246582\n"
          ]
        }
      ],
      "source": [
        "# For Shingling class methods\n",
        "t_shin_jacc_sim = np.zeros(len(n_of_docs))\n",
        "\n",
        "for index in range(len(n_of_docs)):\n",
        "  # start timer\n",
        "  t_start = time.time()\n",
        "  n_docs = n_of_docs[index]\n",
        "  # Get n_docs documents from the datase\n",
        "  data_ls = random.sample(list(df['text']),n_docs)\n",
        "  # Computes shingles\n",
        "  shingles_list = list_shingles(data_ls)\n",
        "  # Create the Similarity Matrix, storing the percentage of similarity document by document\n",
        "  simil_mat = np.zeros((n_docs,n_docs))\n",
        "  # Count similar documents\n",
        "  total = 0\n",
        "  # Compute the Jaccard Similarity\n",
        "  for i in range(n_docs-1):\n",
        "    for j in range(n_docs-1):\n",
        "      simil_mat[i,j] = jaccard_similarity(shingles_list[i],shingles_list[j])\n",
        "      if i==j: simil_mat[i,j] = 0 # to remove ones in diagonal\n",
        "      if simil_mat[i,j]>=0.14: total +=1\n",
        "  t_stop = time.time() - t_start\n",
        "  t_shin_jacc_sim[index] = t_stop\n",
        "\n",
        "  print(\"---\\nNumber of documents: \", n_docs, \"\\nSimilar documents: \", total, \"\\nTime spent: \", t_stop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxLCaigNSF_P"
      },
      "outputs": [],
      "source": [
        "# Plot\n",
        "plt.plot(n_of_docs, t_shin_jacc_sim, label='Jaccard on shingles')\n",
        "plt.xlabel('Number of documents')\n",
        "plt.ylabel('Execution time (s)')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtZLy1ozNGFj"
      },
      "source": [
        "### MinHashing Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "ihUcergUEeap",
        "outputId": "503bc18e-d284-42bb-864a-c07242e84f05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of documents:  10 \n",
            "Similar documents:  0 \n",
            "Time spent:  1.7097885608673096\n",
            "Number of documents:  100 \n",
            "Similar documents:  0 \n",
            "Time spent:  17.519752740859985\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-1450ca8d49ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_docs\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_docs\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m       \u001b[0msimil_mat_sign\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompare_signatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignatures_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignatures_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK_MINHASH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msimil_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;31m# to remove ones in diagonal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0msimil_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m0.14\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 100 is out of bounds for axis 1 with size 100"
          ]
        }
      ],
      "source": [
        "# For MinHashing\n",
        "t_minh_jacc_sim = np.zeros(len(n_of_docs))\n",
        "\n",
        "for index in range(len(n_of_docs)):\n",
        "  # start timer\n",
        "  t_start = time.time()\n",
        "  n_docs = n_of_docs[index]\n",
        "  # Get n_docs documents from the datase\n",
        "  data_ls = random.sample(list(df['text']),n_docs)\n",
        "  # Compute shingles\n",
        "  shingles_list = list_shingles(data_ls)\n",
        "  # Computes signature matrix\n",
        "  signatures_mat = minhash(shingles_list)\n",
        "  # Create the Similarity Matrix, storing the percentage of similarity document by document\n",
        "  simil_mat = np.zeros((n_docs,n_docs))\n",
        "  # Count similar documents\n",
        "  total = 0\n",
        "  # Compute the Jaccard Similarity\n",
        "  for i in range(n_docs-1):\n",
        "    for j in range(n_docs-1):\n",
        "      simil_mat_sign[i,j] = compare_signatures(set(signatures_mat[:,i]),set(signatures_mat[:,j]), K_MINHASH)\n",
        "      if i==j: simil_mat[i,j] = 0 # to remove ones in diagonal\n",
        "      if simil_mat[i,j]>=0.14: total +=1\n",
        "  t_stop = time.time() - t_start\n",
        "  t_minh_jacc_sim[index] = t_stop\n",
        "\n",
        "  print(\"Number of documents: \", n_docs, \"\\nSimilar documents: \", total, \"\\nTime spent: \", t_stop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlm71fU8Eh7K"
      },
      "outputs": [],
      "source": [
        "# For LSH\n",
        "t_lsh_jacc_sim = np.zeros(len(n_of_docs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58FUkHFZEkUm"
      },
      "outputs": [],
      "source": [
        "# Graphs"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
